{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqbnqM-TATqZ",
        "outputId": "08216d1c-bf6c-4572-d3e7-ec01f1d46c86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znkReliSAXX0"
      },
      "outputs": [],
      "source": [
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDSnP3EsMYok",
        "outputId": "245d125d-8c98-4af6-c48d-2baa67fdbc22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "#path = \"/content/drive/MyDrive/MyDatasets/network.dat\"\n",
        "#path = \"/content/drive/MyDrive/MyDatasets/network.dat\"\n",
        "#path = \"/content/drive/MyDrive/MyDatasets/2.Riskmap.csv\"\n",
        "#path = \"/content/drive/MyDrive/MyDatasets/10.power.txt\"\n",
        "#path = \"/content/drive/MyDrive/1.KARATE.csv\"\n",
        "path = \"/content/drive/MyDrive/12.sawmill.csv\"\n",
        "#3.polbooks.txt\"11.Strike.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUx2ITMaHVMi"
      },
      "outputs": [],
      "source": [
        "# PATH = \"/content/drive/MyDrive/3.polbooks.txt\"\n",
        "PATH = \"/content/drive/MyDrive/12.sawmill.csv\"\n",
        "GT_COMM_PATH = \"/content/drive/MyDrive/15rw_comm_t03.txt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "id": "gcYkWU8wWDxm",
        "outputId": "49b6e8b8-e9ac-4e4d-bee2-1f22b2b3b7b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph loaded: 333045 nodes, 245861 edges\n",
            "Modularity: 1.0000\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 496\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModularity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQ\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# print(f\"NMI:        {NMI:.4f}\")\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# Visualization\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;66;03m#plot_communities(G, gt_labels, \"Ground Truth Communities (Karate)\")\u001b[39;00m\n\u001b[1;32m--> 496\u001b[0m \u001b[43mplot_communities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDetected Communities (SAMOOH)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Display communities as list of lists\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------\u001b[39;00m\n\u001b[0;32m    500\u001b[0m communities \u001b[38;5;241m=\u001b[39m labels_to_communities(labels)\n",
            "Cell \u001b[1;32mIn[2], line 461\u001b[0m, in \u001b[0;36mplot_communities\u001b[1;34m(G, labels, title)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_communities\u001b[39m(G, labels, title):\n\u001b[1;32m--> 461\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspring_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m     comms \u001b[38;5;241m=\u001b[39m labels_to_communities(labels)\n\u001b[0;32m    464\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
            "File \u001b[1;32mc:\\Users\\ariha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\networkx\\utils\\decorators.py:788\u001b[0m, in \u001b[0;36margmap.__call__.<locals>.func\u001b[1;34m(_argmap__wrapper, *args, **kwargs)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(\u001b[38;5;241m*\u001b[39margs, __wrapper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43margmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__wrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 12:4\u001b[0m, in \u001b[0;36margmap_spring_layout_9\u001b[1;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ariha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\networkx\\drawing\\layout.py:486\u001b[0m, in \u001b[0;36mspring_layout\u001b[1;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[0;32m    484\u001b[0m         nnodes, _ \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    485\u001b[0m         k \u001b[38;5;241m=\u001b[39m dom_size \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(nnodes)\n\u001b[1;32m--> 486\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[43m_sparse_fruchterman_reingold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m     A \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mto_numpy_array(G, weight\u001b[38;5;241m=\u001b[39mweight)\n",
            "File \u001b[1;32mc:\\Users\\ariha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\networkx\\utils\\decorators.py:788\u001b[0m, in \u001b[0;36margmap.__call__.<locals>.func\u001b[1;34m(_argmap__wrapper, *args, **kwargs)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(\u001b[38;5;241m*\u001b[39margs, __wrapper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43margmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__wrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 20:4\u001b[0m, in \u001b[0;36margmap__sparse_fruchterman_reingold_17\u001b[1;34m(A, k, pos, fixed, iterations, threshold, dim, seed)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ariha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\networkx\\drawing\\layout.py:629\u001b[0m, in \u001b[0;36m_sparse_fruchterman_reingold\u001b[1;34m(A, k, pos, fixed, iterations, threshold, dim, seed)\u001b[0m\n\u001b[0;32m    625\u001b[0m     Ai \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mgetrowview(i)\u001b[38;5;241m.\u001b[39mtoarray()  \u001b[38;5;66;03m# TODO: revisit w/ sparse 1D container\u001b[39;00m\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# displacement \"force\"\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     displacement[:, i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdistance\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mAi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdistance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m--> 629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# update positions\u001b[39;00m\n\u001b[0;32m    631\u001b[0m length \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt((displacement\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
            "File \u001b[1;32mc:\\Users\\ariha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:47\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_minimum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#NEW_HPLMC\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from math import sqrt\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "from networkx.algorithms.community.quality import modularity\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Load Karate graph\n",
        "# -------------------------------------------------\n",
        "# G = nx.karate_club_graph()\n",
        "# print(\"Graph loaded:\", G.number_of_nodes(), \"nodes,\", G.number_of_edges(), \"edges\")\n",
        "\n",
        "# # Ground truth\n",
        "# gt_labels = {}\n",
        "# for n, d in G.nodes(data=True):\n",
        "#     gt_labels[n] = 0 if d[\"club\"] == \"Mr. Hi\" else 1\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Load Karate graph from CSV\n",
        "# -------------------------------------------------\n",
        "EDGE_PATH = \"boston.csv\" # 4.football.csv,, 2.Riskmap.csv, 3.polbooks.txt, 11.Strike.csv, 12. sawmill.csv\n",
        "#GT_COMM_PATH = \"/content/drive/MyDrive/15rw_comm_t05.txt\"\n",
        "\n",
        "def load_graph_from_csv(path):\n",
        "    G = nx.Graph()\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.replace(\",\", \" \").split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            try:\n",
        "                u = int(parts[0])\n",
        "                v = int(parts[1])\n",
        "            except:\n",
        "                continue\n",
        "            if u != v:\n",
        "                G.add_edge(u, v)\n",
        "    return G\n",
        "\n",
        "G = load_graph_from_csv(EDGE_PATH)\n",
        "print(\"Graph loaded:\", G.number_of_nodes(), \"nodes,\", G.number_of_edges(), \"edges\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Load ground-truth communities\n",
        "# Format:\n",
        "# each line = one community\n",
        "# -------------------------------------------------\n",
        "def load_ground_truth(path):\n",
        "    gt = {}\n",
        "    with open(path, \"r\") as f:\n",
        "        for label, line in enumerate(f):\n",
        "            nodes = line.strip().split()\n",
        "            for n in nodes:\n",
        "                gt[int(n)] = label\n",
        "    return gt\n",
        "\n",
        "#gt_labels = load_ground_truth(GT_COMM_PATH)\n",
        "#print(\"Ground truth labels loaded:\", len(gt_labels))\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Sanity check\n",
        "# -------------------------------------------------\n",
        "# missing = set(G.nodes()) - set(gt_labels.keys())\n",
        "# if missing:\n",
        "#     print(\"WARNING: Missing GT labels for nodes:\", missing)\n",
        "# #else:\n",
        "#     #print(\"Ground truth covers all nodes.\")\n",
        "\n",
        "\n",
        "# Similarity measures\n",
        "# -------------------------------------------------\n",
        "nbrs = {n: set(G.neighbors(n)) for n in G.nodes()}\n",
        "deg = dict(G.degree())\n",
        "\n",
        "def sim_adamic_adar(v, u):\n",
        "    return sum(1 / np.log(deg[z]) for z in nbrs[v] & nbrs[u] if deg[z] > 1)\n",
        "\n",
        "def sim_resource_allocation(v, u):\n",
        "    return sum(1 / deg[z] for z in nbrs[v] & nbrs[u] if deg[z] > 0)\n",
        "\n",
        "def sim_salton(v, u):\n",
        "    return len(nbrs[v] & nbrs[u]) / sqrt(max(1, deg[v] * deg[u]))\n",
        "\n",
        "def sim_preferential_attachment(v, u):\n",
        "    return deg[v] * deg[u]\n",
        "\n",
        "SIM_FUNCS = {\n",
        "    \"adamic_adar\": sim_adamic_adar,\n",
        "    \"preferential_attachment\": sim_preferential_attachment,\n",
        "    \"resource_allocation\": sim_resource_allocation,\n",
        "    \"salton\": sim_salton\n",
        "}\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Reweighted graph construction\n",
        "# w(u,v) = adjacency + α·triangles + β·3-paths\n",
        "# -------------------------------------------------\n",
        "# def compute_reweighted_graph(G, alpha=1.0, beta=0.5):\n",
        "#     G_rw = nx.Graph()\n",
        "#     G_rw.add_nodes_from(G.nodes())\n",
        "\n",
        "#     # --- triangle counts ---\n",
        "#     tri_weight = defaultdict(int)\n",
        "#     for v in G.nodes():\n",
        "#         neigh = list(G.neighbors(v))\n",
        "#         for i in range(len(neigh)):\n",
        "#             for j in range(i + 1, len(neigh)):\n",
        "#                 u, w = neigh[i], neigh[j]\n",
        "#                 if G.has_edge(u, w):\n",
        "#                     tri_weight[(u, v)] += 1\n",
        "#                     tri_weight[(v, w)] += 1\n",
        "#                     tri_weight[(u, w)] += 1\n",
        "\n",
        "#     # --- 3-path counts ---\n",
        "#     path_weight = defaultdict(int)\n",
        "#     for x in G.nodes():\n",
        "#         nbrs_x = list(G.neighbors(x))\n",
        "#         for i in range(len(nbrs_x)):\n",
        "#             for j in range(i + 1, len(nbrs_x)):\n",
        "#                 u, v = nbrs_x[i], nbrs_x[j]\n",
        "#                 path_weight[(u, v)] += 1\n",
        "#                 path_weight[(v, u)] += 1\n",
        "\n",
        "#     # --- build weighted edges ---\n",
        "#     for u, v in G.edges():\n",
        "#         w = (\n",
        "#             1.0\n",
        "#             + alpha * tri_weight.get((u, v), 0)\n",
        "#             + beta * path_weight.get((u, v), 0)\n",
        "#         )\n",
        "#         G_rw.add_edge(u, v, weight=w)\n",
        "\n",
        "#     return G_rw\n",
        "# def compute_reweighted_graph(G, alpha=1.0, beta=0.0):\n",
        "#     G_rw = nx.Graph()\n",
        "#     G_rw.add_nodes_from(G.nodes())\n",
        "\n",
        "#     n = G.number_of_nodes()\n",
        "\n",
        "    # --- all-pairs shortest paths ---\n",
        "    # dist = dict(nx.all_pairs_shortest_path_length(G))\n",
        "\n",
        "    # # --- edge closeness centrality ---\n",
        "    # edge_closeness = {}\n",
        "\n",
        "    # for u, v in G.edges():\n",
        "    #     total_dist = 0\n",
        "    #     for x in G.nodes():\n",
        "    #         total_dist += min(dist[u][x], dist[v][x])\n",
        "\n",
        "    #     # avoid division by zero (theoretically impossible in connected graphs)\n",
        "    #     edge_closeness[(u, v)] = n / total_dist if total_dist > 0 else 0.0\n",
        "    #     edge_closeness[(v, u)] = edge_closeness[(u, v)]\n",
        "\n",
        "    # # --- build weighted edges ---\n",
        "    # for u, v in G.edges():\n",
        "    #     w = 1.0 + alpha * edge_closeness[(u, v)]\n",
        "    #     G_rw.add_edge(u, v, weight=w)\n",
        "\n",
        "    # return G_rw\n",
        "\n",
        "\n",
        "# def compute_reweighted_graph(G, alpha=1.0, beta=0.0):\n",
        "#     G_rw = nx.Graph()\n",
        "#     G_rw.add_nodes_from(G.nodes())\n",
        "\n",
        "#     n = G.number_of_nodes()\n",
        "#     deg = dict(G.degree())\n",
        "#     nbrs = {u: set(G.neighbors(u)) for u in G.nodes()}\n",
        "\n",
        "#     # --- shortest paths ---\n",
        "#     dist = dict(nx.all_pairs_shortest_path_length(G))\n",
        "\n",
        "#     for u, v in G.edges():\n",
        "#         # ----- 1. softened edge closeness -----\n",
        "#         total_dist = 0\n",
        "#         for x in G.nodes():\n",
        "#             total_dist += min(dist[u][x], dist[v][x])\n",
        "#         Ce = 1.0 / total_dist\n",
        "\n",
        "#         # ----- 2. triangle density -----\n",
        "#         tri = len(nbrs[u] & nbrs[v])\n",
        "#         T = (tri + 1) / np.sqrt(max(1, deg[u] * deg[v]))\n",
        "\n",
        "#         # ----- 3. hub regularization -----\n",
        "#         R = 1.0 / np.log(2 + deg[u] + deg[v])\n",
        "\n",
        "#         # ----- final weight -----\n",
        "#         w = 1.0 + alpha * Ce * T * R\n",
        "\n",
        "#         G_rw.add_edge(u, v, weight=w)\n",
        "\n",
        "#     return G_rw\n",
        "# def compute_reweighted_graph(G, alpha=1.0, beta=0.0):\n",
        "#     G_rw = nx.Graph()\n",
        "#     G_rw.add_nodes_from(G.nodes())\n",
        "\n",
        "#     deg = dict(G.degree())\n",
        "#     nbrs = {u: set(G.neighbors(u)) for u in G.nodes()}\n",
        "\n",
        "#     # all-pairs shortest paths (small graphs OK)\n",
        "#     dist = dict(nx.all_pairs_shortest_path_length(G))\n",
        "\n",
        "#     for u, v in G.edges():\n",
        "#         # ----- local cohesion (dominant) -----\n",
        "#         shared = len(nbrs[u] & nbrs[v])\n",
        "#         T = (shared + 1) / max(1, min(deg[u], deg[v]))\n",
        "\n",
        "#         # ----- gated global term -----\n",
        "#         total_dist = 0\n",
        "#         for x in G.nodes():\n",
        "#             total_dist += min(dist[u][x], dist[v][x])\n",
        "#         G_term = 1.0 / (1.0 + total_dist)\n",
        "\n",
        "#         # ----- final weight -----\n",
        "#         w = 1.0 + alpha * T * G_term\n",
        "#         G_rw.add_edge(u, v, weight=w)\n",
        "\n",
        "#     return G_rw\n",
        "\n",
        "# def compute_reweighted_graph(G, alpha=1.0, beta=0.5):\n",
        "#     G_rw = nx.Graph()\n",
        "#     G_rw.add_nodes_from(G.nodes())\n",
        "\n",
        "#     deg = dict(G.degree())\n",
        "#     nbrs = {u: set(G.neighbors(u)) for u in G.nodes()}\n",
        "\n",
        "#     # --- triangle counts (edge-based) ---\n",
        "#     tri_weight = defaultdict(int)\n",
        "#     for v in G.nodes():\n",
        "#         neigh = list(G.neighbors(v))\n",
        "#         for i in range(len(neigh)):\n",
        "#             for j in range(i + 1, len(neigh)):\n",
        "#                 u, w = neigh[i], neigh[j]\n",
        "#                 if G.has_edge(u, w):\n",
        "#                     tri_weight[(u, v)] += 1\n",
        "#                     tri_weight[(v, w)] += 1\n",
        "#                     tri_weight[(u, w)] += 1\n",
        "\n",
        "#     # --- 3-path counts ---\n",
        "#     path_weight = defaultdict(int)\n",
        "#     for x in G.nodes():\n",
        "#         nbrs_x = list(G.neighbors(x))\n",
        "#         for i in range(len(nbrs_x)):\n",
        "#             for j in range(i + 1, len(nbrs_x)):\n",
        "#                 u, v = nbrs_x[i], nbrs_x[j]\n",
        "#                 path_weight[(u, v)] += 1\n",
        "#                 path_weight[(v, u)] += 1\n",
        "\n",
        "#     # --- build weighted edges ---\n",
        "#     for u, v in G.edges():\n",
        "#         T = (tri_weight.get((u, v), 0) + 1) / np.sqrt(max(1, deg[u] * deg[v]))\n",
        "#         P = path_weight.get((u, v), 0) / max(1, min(deg[u], deg[v]))\n",
        "\n",
        "#         w = 1.0 + alpha * T + beta * P\n",
        "#         G_rw.add_edge(u, v, weight=w)\n",
        "\n",
        "#     return G_rw\n",
        "\n",
        "\n",
        "# def compute_reweighted_graph(G, alpha=1.0, beta=0.5):\n",
        "#     G_rw = nx.Graph()\n",
        "#     G_rw.add_nodes_from(G.nodes())\n",
        "\n",
        "#     deg = dict(G.degree())\n",
        "#     nbrs = {u: set(G.neighbors(u)) for u in G.nodes()}\n",
        "\n",
        "#     # --- triangle counts ---\n",
        "#     tri_weight = defaultdict(int)\n",
        "#     for v in G.nodes():\n",
        "#         neigh = list(G.neighbors(v))\n",
        "#         for i in range(len(neigh)):\n",
        "#             for j in range(i + 1, len(neigh)):\n",
        "#                 u, w = neigh[i], neigh[j]\n",
        "#                 if G.has_edge(u, w):\n",
        "#                     tri_weight[(u, v)] += 1\n",
        "#                     tri_weight[(v, w)] += 1\n",
        "#                     tri_weight[(u, w)] += 1\n",
        "\n",
        "#     # --- 3-path counts ---\n",
        "#     path_weight = defaultdict(int)\n",
        "#     for x in G.nodes():\n",
        "#         nbrs_x = list(G.neighbors(x))\n",
        "#         for i in range(len(nbrs_x)):\n",
        "#             for j in range(i + 1, len(nbrs_x)):\n",
        "#                 u, v = nbrs_x[i], nbrs_x[j]\n",
        "#                 path_weight[(u, v)] += 1\n",
        "#                 path_weight[(v, u)] += 1\n",
        "\n",
        "#     # --- build weighted edges ---\n",
        "#     for u, v in G.edges():\n",
        "#         T_raw = tri_weight.get((u, v), 0)\n",
        "#         P_raw = path_weight.get((u, v), 0)\n",
        "\n",
        "#         # normalized triangle term\n",
        "#         T = (T_raw + 1) / np.sqrt(max(1, deg[u] * deg[v]))\n",
        "\n",
        "#         # gated 3-path term\n",
        "#         P = P_raw / max(1, min(deg[u], deg[v]))\n",
        "\n",
        "#         # edge embeddedness (conductance control)\n",
        "#         emb = T_raw / (min(deg[u] - 1, deg[v] - 1) + 1)\n",
        "\n",
        "#         w = 1.0 + (alpha * T + beta * P) * emb\n",
        "#         G_rw.add_edge(u, v, weight=w)\n",
        "\n",
        "#     return G_rw\n",
        "\n",
        "def compute_reweighted_graph(G):\n",
        "    G_rw = nx.Graph()\n",
        "    G_rw.add_nodes_from(G.nodes())\n",
        "\n",
        "    deg = dict(G.degree())\n",
        "    nbrs = {u: set(G.neighbors(u)) for u in G.nodes()}\n",
        "\n",
        "    for u, v in G.edges():\n",
        "        common = nbrs[u] & nbrs[v]\n",
        "        c = len(common)\n",
        "\n",
        "        # --- Triangle probability (Jaccard) ---\n",
        "        union_size = len(nbrs[u] | nbrs[v])\n",
        "        T = c / union_size if union_size > 0 else 0.0\n",
        "\n",
        "        # --- Two-step proximity (local closeness surrogate) ---\n",
        "        if c > 0:\n",
        "            P = sum(1 / deg[x] for x in common) / c\n",
        "        else:\n",
        "            P = 0.0\n",
        "\n",
        "        # --- Embeddedness damping ---\n",
        "        E = c / min(deg[u], deg[v]) if min(deg[u], deg[v]) > 0 else 0.0\n",
        "\n",
        "        w = 1.0 + (T + P) * E\n",
        "        G_rw.add_edge(u, v, weight=w)\n",
        "\n",
        "    return G_rw\n",
        "\n",
        "\n",
        "# def compute_reweighted_graph(G, alpha=1.0, beta=0.5):\n",
        "#     G_rw = nx.Graph()\n",
        "#     G_rw.add_nodes_from(G.nodes())\n",
        "\n",
        "#     deg = dict(G.degree())\n",
        "#     nbrs = {u: set(G.neighbors(u)) for u in G.nodes()}\n",
        "\n",
        "#     # --- triangle counts ---\n",
        "#     tri_weight = defaultdict(int)\n",
        "#     for v in G.nodes():\n",
        "#         neigh = list(G.neighbors(v))\n",
        "#         for i in range(len(neigh)):\n",
        "#             for j in range(i + 1, len(neigh)):\n",
        "#                 u, w = neigh[i], neigh[j]\n",
        "#                 if G.has_edge(u, w):\n",
        "#                     tri_weight[(u, v)] += 1\n",
        "#                     tri_weight[(v, w)] += 1\n",
        "#                     tri_weight[(u, w)] += 1\n",
        "\n",
        "#     # --- 3-path counts ---\n",
        "#     path_weight = defaultdict(int)\n",
        "#     for x in G.nodes():\n",
        "#         nbrs_x = list(G.neighbors(x))\n",
        "#         for i in range(len(nbrs_x)):\n",
        "#             for j in range(i + 1, len(nbrs_x)):\n",
        "#                 u, v = nbrs_x[i], nbrs_x[j]\n",
        "#                 path_weight[(u, v)] += 1\n",
        "#                 path_weight[(v, u)] += 1\n",
        "\n",
        "#     # --- build weighted edges ---\n",
        "#     for u, v in G.edges():\n",
        "#         T_raw = tri_weight.get((u, v), 0)\n",
        "#         P_raw = path_weight.get((u, v), 0)\n",
        "\n",
        "#         # triangle term (always active)\n",
        "#         T = (T_raw + 1) / np.sqrt(max(1, deg[u] * deg[v]))\n",
        "\n",
        "#         # gated 3-path term (ONLY if triangles exist)\n",
        "#         gate = T_raw / (T_raw + 1)\n",
        "#         P = gate * (P_raw / max(1, min(deg[u], deg[v])))\n",
        "\n",
        "#         w = 1.0 + alpha * T + beta * P\n",
        "#         G_rw.add_edge(u, v, weight=w)\n",
        "\n",
        "#     return G_rw\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# SAMOOH (Label Propagation on Reweighted Graph)\n",
        "# -------------------------------------------------\n",
        "def h_lpmc(G_rw, sim_func, max_iter=100):\n",
        "    deg = dict(G_rw.degree(weight=\"weight\"))\n",
        "    order = sorted(G_rw.nodes(), key=lambda x: deg[x], reverse=True)\n",
        "\n",
        "    labels = {v: v for v in G_rw.nodes()}\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        changed = False\n",
        "        for v in order:\n",
        "            best_score = -1e18\n",
        "            best_nbr = None\n",
        "\n",
        "            for u in G_rw.neighbors(v):\n",
        "                w = G_rw[v][u][\"weight\"]\n",
        "                score = w * sim_func(v, u)\n",
        "                #score = w\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_nbr = u\n",
        "\n",
        "            if best_nbr is not None and labels[v] != labels[best_nbr]:\n",
        "                labels[v] = labels[best_nbr]\n",
        "                changed = True\n",
        "\n",
        "        if not changed:\n",
        "            break\n",
        "\n",
        "    return labels\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Conductance (UNCHANGED)\n",
        "# -------------------------------------------------\n",
        "def conductance_of_partition(G, partition):\n",
        "    degs = dict(G.degree())\n",
        "    conds = []\n",
        "    for S in partition:\n",
        "        if len(S) == 0 or len(S) == G.number_of_nodes():\n",
        "            continue\n",
        "        cut = 0\n",
        "        volS = 0\n",
        "        for u in S:\n",
        "            volS += degs[u]\n",
        "            for v in G.neighbors(u):\n",
        "                if v not in S:\n",
        "                    cut += 1\n",
        "        volSc = sum(degs[v] for v in G.nodes() if v not in S)\n",
        "        denom = min(volS, volSc)\n",
        "        if denom > 0:\n",
        "            conds.append(cut / denom)\n",
        "    return float(np.mean(conds)) if conds else np.nan\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Utilities\n",
        "# -------------------------------------------------\n",
        "def labels_to_communities(labels):\n",
        "    comms = defaultdict(list)\n",
        "    for n, l in labels.items():\n",
        "        comms[l].append(n)\n",
        "    return list(comms.values())\n",
        "\n",
        "def plot_communities(G, labels, title):\n",
        "    pos = nx.spring_layout(G, seed=42)\n",
        "    comms = labels_to_communities(labels)\n",
        "\n",
        "    plt.figure(figsize=(7, 6))\n",
        "    for i, c in enumerate(comms):\n",
        "        nx.draw_networkx_nodes(G, pos, nodelist=c, node_size=300, label=f\"C{i}\")\n",
        "    nx.draw_networkx_edges(G, pos, alpha=0.4)\n",
        "    nx.draw_networkx_labels(G, pos, font_size=9)\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Run algorithm\n",
        "# -------------------------------------------------\n",
        "G_rw = compute_reweighted_graph(G)\n",
        "labels = h_lpmc(G_rw, sim_adamic_adar)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Evaluation\n",
        "# -------------------------------------------------\n",
        "pred = np.array([labels[n] for n in sorted(G.nodes())])\n",
        "#gt = np.array([gt_labels[n] for n in sorted(G.nodes())])\n",
        "\n",
        "Q = modularity(G, labels_to_communities(labels))\n",
        "# NMI = normalized_mutual_info_score(gt, pred)\n",
        "\n",
        "print(f\"Modularity: {Q:.4f}\")\n",
        "# print(f\"NMI:        {NMI:.4f}\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Visualization\n",
        "# -------------------------------------------------\n",
        "#plot_communities(G, gt_labels, \"Ground Truth Communities (Karate)\")\n",
        "plot_communities(G, labels, \"Detected Communities (SAMOOH)\")\n",
        "# -------------------------------------------------\n",
        "# Display communities as list of lists\n",
        "# -------------------------------------------------\n",
        "communities = labels_to_communities(labels)\n",
        "\n",
        "\n",
        "# Run experiments\n",
        "# -------------------------------------------------\n",
        "G_rw = compute_reweighted_graph(G)\n",
        "records = []\n",
        "\n",
        "for sim_name, sim_func in SIM_FUNCS.items():\n",
        "    labels = h_lpmc(G_rw, sim_func)\n",
        "    partition = labels_to_communities(labels)\n",
        "    modularity_val = modularity(G, partition)\n",
        "    pred = np.array([labels[n] for n in sorted(G.nodes())])\n",
        "    #gt = np.array([gt_labels[n] for n in sorted(G.nodes())])\n",
        "    #nmi_val = normalized_mutual_info_score(gt, pred)\n",
        "    cond_val = conductance_of_partition(G, partition)\n",
        "\n",
        "    records.append({\n",
        "        \"similarity\": sim_name,\n",
        "        \"modularity_mean\": modularity_val,\n",
        "        # \"nmi_mean\": nmi_val,\n",
        "        \"conductance_mean\": cond_val\n",
        "    })\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Display summary\n",
        "# -------------------------------------------------\n",
        "df = pd.DataFrame(records)\n",
        "#df = df[[\"similarity\", \"modularity_mean\", \"nmi_mean\", \"conductance_mean\"]]\n",
        "df = df[[\"similarity\", \"modularity_mean\", \"conductance_mean\"]]\n",
        "print(\"\\nSummary Metrics for All Similarity Functions:\\n\")\n",
        "print(df.to_string(index=False))\n",
        "# Display communities as list of lists\n",
        "\n",
        "# sort for readability\n",
        "communities = [sorted(c) for c in communities]\n",
        "communities = sorted(communities, key=len, reverse=True)\n",
        "\n",
        "print(\"\\nDetected Communities (list of lists):\")\n",
        "for i, c in enumerate(communities):\n",
        "    print(f\"Community {i}: {c}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
